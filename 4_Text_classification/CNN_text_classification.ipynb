{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/onlyabhilash/NLP-Projects/blob/main/4_Text_classification/CNN_text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7XRkEiW9E0w"
      },
      "source": [
        "## Text classification by Convolution Neural Networks\n",
        "### Model structure and dimensions\n",
        "- Input embedding layer\n",
        "    - `(batch_size, seq_length, embedding_size, 1)`\n",
        "- `for f in filter_sizes`\n",
        "    - Convolution\n",
        "        - Filter shape: `(f, embedding_size, 1, num_filters)`\n",
        "        - Output shape: `(batch_size, seq_length-f+1, 1, num_filters)` with `stride=1`\n",
        "    - Max-pool\n",
        "        - `ksize=[1, seq_length-f+1, 1, 1]`\n",
        "        - Output shape: reshape `(batch_size, 1, 1, num_filters)` to `(batch_size, num_filters)`\n",
        "- Concatenate outputs of all filters together\n",
        "    - `tf.concat(xx, axis=-1)`\n",
        "    - Output shape: `(batch_size, len(filter_sizes)*num_filters)`\n",
        "- FC1 with drop-out\n",
        "    - Output shape: `(batch_size, len(filter_sizes)*num_filters)`\n",
        "- FC2\n",
        "    - Output shape: `(batch_size, num_classes)`\n",
        "\n",
        "### How to add He initialization ?\n",
        "- `he_init = tf.contrib.layers.variance_scaling_initializer()`\n",
        "- `fc1 = tf.contrib.layers.fully_connected(inputs=conv_res, ..., weights_initializer=he_init)\n",
        "\n",
        "### How to perform L2 regularization using advanced API `tf.contrib.layers` ? \n",
        "- `tf.contrib.layers.fully_connected(inputs, num_outputs, weights_regularizer=tf.contrib.layers.l2_regularizer(scale))`\n",
        "- `reg_ws = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)`\n",
        "- `cost = cross_entropy_cost + tf.reduce_sum(reg_ws)` # adds to total cost\n",
        "\n",
        "References:  \n",
        "[1] https://stackoverflow.com/questions/37107223/how-to-add-regularizations-in-tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "YrB41wmp9E04"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import codecs\n",
        "import itertools\n",
        "from collections import Counter\n",
        "from random import shuffle\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMRCRIUA9E06"
      },
      "source": [
        "Class `DataGenerator` is used to read input files, convert words to index and generate batch training or testing data. \n",
        "\n",
        "Since the CNN input has fixed length. `Arguments.MAX_SEQ_LENGTH` records the maximum sequence length in training data. All sequences that lengths less than `Arguments.MAX_SEQ_LENGTH` are padded to `Arguments.MAX_SEQ_LENGTH`.  \n",
        "\n",
        "Two extra words are introduced, `PAD` for padding shorter sequences and `OOV` for representing out-of-vocabulary words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "AJBmBarJ9E07"
      },
      "outputs": [],
      "source": [
        "class DataGenerator():\n",
        "    \"\"\"\n",
        "    reading each training and testing files, and generating batch data.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, args):\n",
        "        self.folder_path = args.FOLDER_PATH\n",
        "        self.batch_size = args.BATCH_SIZE\n",
        "        self.vocab_size = args.VOCAB_SIZE\n",
        "        self.num_epoch = args.NUM_EPOCH\n",
        "        self.read_build_input()\n",
        "        self.single_generator_training = self.generate_sample_training()\n",
        "        self.single_generator_testing = self.generate_sample_testing()\n",
        "        self.label_dict = {0:'auto', 1:'business', 2:'IT', 3:'health', 4:'sports', 5:'yule'}\n",
        "    \n",
        "    def one_hot_encode(self, x, n_classes=6):\n",
        "        return np.eye(n_classes)[[x]][0]\n",
        "        \n",
        "    def read_build_input(self):\n",
        "        training_src = []\n",
        "        testing_src = []\n",
        "        article_len = []\n",
        "\n",
        "        for cur_category in range(1, 7):\n",
        "            \n",
        "            print('parsing file >>>>>>>>>>>>>>> ', cur_category)\n",
        "            print('-'*100)\n",
        "            \n",
        "            training_input_file = codecs.open(filename=os.path.join(self.folder_path, 'training_' + str(cur_category) + '.cs'), mode='r', encoding='utf-8')\n",
        "            for tmp_line in training_input_file:\n",
        "                training_src.append((tmp_line.split(), cur_category-1))\n",
        "                article_len.append(len(tmp_line.split()))\n",
        "\n",
        "            testing_input_file = codecs.open(filename=os.path.join(self.folder_path, 'testing_' + str(cur_category) + '.cs'), mode='r', encoding='utf-8')\n",
        "            for tmp_line in testing_input_file:\n",
        "                testing_src.append((tmp_line.split(), cur_category-1))                \n",
        "\n",
        "        shuffle(training_src)\n",
        "        shuffle(testing_src)\n",
        "        \n",
        "        args.MAX_SEQ_LENGTH = max(article_len)\n",
        "        print('='*100)\n",
        "        print('Size of training data:', len(training_src))\n",
        "        print('Size of testing data:', len(testing_src))\n",
        "        print('Maximum length of training articles', args.MAX_SEQ_LENGTH)\n",
        "    \n",
        "        self.TRAINING_SIZE = len(training_src)\n",
        "        args.TESTING_SIZE = len(testing_src)\n",
        "        \n",
        "        training_X_src = [pair[0] for pair in training_src]\n",
        "        testing_X_src = [pair[0] for pair in testing_src]\n",
        "        all_data = list(itertools.chain.from_iterable(training_X_src + testing_X_src))\n",
        "        word_counter = Counter(all_data).most_common(self.vocab_size-2)\n",
        "        del all_data\n",
        "        \n",
        "        print('='*100)\n",
        "        if os.path.isfile(args.MAPFILE):\n",
        "            print('Reload word2idx')\n",
        "            with open(args.MAPFILE, 'rb') as f:\n",
        "                self.word2idx, self.idx2word = pickle.load(f)\n",
        "        else:\n",
        "            print('top 10 frequent words:')\n",
        "            print(word_counter[0:10])\n",
        "            self.word2idx = {val[0]: idx+1 for idx, val in enumerate(word_counter)}\n",
        "            self.word2idx['PAD'] = 0 # padding word\n",
        "            self.word2idx['OOV'] = self.vocab_size - 1 # out-of-vocabulary\n",
        "            self.idx2word = dict(zip(self.word2idx.values(), self.word2idx.keys()))\n",
        "            print('Total vocabulary size:{}'.format(len(self.word2idx)))            \n",
        "        \n",
        "            with open(args.MAPFILE, 'wb') as f:\n",
        "                pickle.dump([self.word2idx, self.idx2word], f)\n",
        "        \n",
        "        self.training = [([self.word2idx[w] if w in self.word2idx else self.word2idx['OOV'] for w in tmp_pair[0][0:args.MAX_SEQ_LENGTH]], tmp_pair[1]) for tmp_pair in training_src]\n",
        "        self.testing_ori =  [([self.word2idx[w] if w in self.word2idx else self.word2idx['OOV'] for w in tmp_pair[0][0:args.MAX_SEQ_LENGTH]], tmp_pair[1]) for tmp_pair in testing_src]\n",
        "        self.testing = [(tmp_pair[0] + [self.word2idx['PAD']] * (args.MAX_SEQ_LENGTH - len(tmp_pair[0])), tmp_pair[1]) if len(tmp_pair[0]) < args.MAX_SEQ_LENGTH else tmp_pair for tmp_pair in self.testing_ori]\n",
        "    \n",
        "    def generate_sample_training(self):\n",
        "        \"\"\"\n",
        "        If len(each article) < self.max_seq_len:\n",
        "            padding them with 0\n",
        "        else:\n",
        "            truncating them to self.max_seq_len\n",
        "        \"\"\"\n",
        "        outer_index = 0\n",
        "        for X_y_pair in itertools.cycle(self.training):  # infinite loop each article\n",
        "            tmp_input_len = len(X_y_pair[0])\n",
        "            if tmp_input_len < args.MAX_SEQ_LENGTH:\n",
        "                input_X = X_y_pair[0] + [self.word2idx['PAD']] * (args.MAX_SEQ_LENGTH - tmp_input_len)\n",
        "            else:\n",
        "                input_X = X_y_pair[0]\n",
        "            \n",
        "            output_y = X_y_pair[1]\n",
        "            if outer_index in [0, 10]:\n",
        "                print('='*100)\n",
        "                print('Training text:', ' '.join([self.idx2word[tmp_id] for tmp_id in input_X]))\n",
        "                print('Training text length:', len(input_X))\n",
        "                print('Training label:', self.label_dict[output_y])\n",
        "                \n",
        "            yield input_X, output_y\n",
        "            outer_index += 1\n",
        "    \n",
        "    def generate_sample_testing(self):\n",
        "        \"\"\"\n",
        "        If len(each article) < self.max_seq_len:\n",
        "            padding them with 0\n",
        "        else:\n",
        "            truncating them to self.max_seq_len\n",
        "        \"\"\"\n",
        "        outer_index = 0\n",
        "        for X_y_pair in self.testing: #itertools.cycle(self.testing):  # infinite loop each article\n",
        "            tmp_input_len = len(X_y_pair[0])\n",
        "            if tmp_input_len < args.MAX_SEQ_LENGTH:\n",
        "                input_X = X_y_pair[0] + [self.word2idx['PAD']] * (args.MAX_SEQ_LENGTH - tmp_input_len)\n",
        "            else:\n",
        "                input_X = X_y_pair[0]\n",
        "            \n",
        "            output_y = X_y_pair[1]\n",
        "            if outer_index in [0, 10]:\n",
        "                print('='*100)\n",
        "                print('Testing text:', ' '.join([self.idx2word[tmp_id] for tmp_id in input_X]))\n",
        "                print('Testing text length:', len(input_X))\n",
        "                print('Testing label:', self.label_dict[output_y])\n",
        "                \n",
        "            yield input_X, output_y\n",
        "            outer_index += 1\n",
        "        \n",
        "\n",
        "    def next_batch_training(self):\n",
        "        input_X_batch = []\n",
        "        output_y_batch = []\n",
        "        for idx in range(self.batch_size):\n",
        "            tmp_X, tmp_y = next(self.single_generator_training)\n",
        "            input_X_batch.append(tmp_X)\n",
        "            output_y_batch.append(self.one_hot_encode(tmp_y))\n",
        "        return np.array(input_X_batch, dtype=np.int32), np.array(output_y_batch, dtype=np.int32)\n",
        "    \n",
        "    def next_testing(self):\n",
        "        testing_X = np.array([tmp_pair[0] for tmp_pair in self.testing], dtype=np.int32)\n",
        "        testing_y = np.array([tmp_pair[1] for tmp_pair in self.testing], dtype=np.int32)\n",
        "        return testing_X, testing_y     \n",
        "    \n",
        "    def testing_data(self):\n",
        "        testing_X = np.array([tmp_pair[0] for tmp_pair in self.testing], dtype=np.int32)\n",
        "        testing_y = np.array([self.one_hot_encode(tmp_pair[1]) for tmp_pair in self.testing], dtype=np.int32)\n",
        "        # here I limit first 10000 samples since my computer memeory limitation\n",
        "        return testing_X[:1000], testing_y[:1000]  \n",
        "        #return testing_X, testing_y  \n",
        "    \n",
        "    \n",
        "    def next_batch_testing(self):\n",
        "        input_X_batch = []\n",
        "        output_y_batch = []\n",
        "        for idx in range(self.batch_size):\n",
        "            tmp_X, tmp_y = next(self.single_generator_testing)\n",
        "            input_X_batch.append(tmp_X)\n",
        "            output_y_batch.append(self.one_hot_encode(tmp_y))\n",
        "        return np.array(input_X_batch, dtype=np.int32), np.array(output_y_batch, dtype=np.int32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elQrHzUo9E0-"
      },
      "source": [
        "Hyper-parameters for this model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "j4TINjLy9E0_"
      },
      "outputs": [],
      "source": [
        "class Arguments:\n",
        "    \"\"\"\n",
        "    main hyper-parameters\n",
        "    \"\"\"\n",
        "    REGULARIZATION = 0.01\n",
        "    EMBED_SIZE = 100 # embedding dimensions\n",
        "    BATCH_SIZE = 32\n",
        "    VOCAB_SIZE = 300000 # vocabulary size\n",
        "    NUM_CLASSES = 6 # number of classes\n",
        "    NUM_FILTERS = 65\n",
        "    # [3, 4, 5] performs best according to http://ruder.io/deep-learning-nlp-best-practices/\n",
        "    FILTER_SIZES = [3, 4, 5]\n",
        "    KEEP_PROB = 0.5\n",
        "    FOLDER_PATH = 'sogou_corpus'\n",
        "    NUM_EPOCH = 10\n",
        "    CHECKPOINTS_DIR = 'text_classification_CNN_model' + '_lambda_' + str(REGULARIZATION)\n",
        "    LOGDIR = 'text_classification_CNN_logdir' + '_lambda_' + str(REGULARIZATION)\n",
        "    MAPFILE = 'cnn_maps.pkl'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xGaM2NS9E0_"
      },
      "source": [
        "### `conv = tf.nn.conv2d(input, filter, strides, padding, data_format='NHWC')`\n",
        "- `conv` and `input` are both 4-D tensor of shape `(batch_size, in_height, in_width, in_channels)`.\n",
        "- `filter` is a 4-D tensor of shape `(filter_height, filter_width, in_channels, out_channels)`.\n",
        "- `strides` is a list of `ints`, `(batch_stride=1, vertical_stride, horizontal_stride, channels_stride=1)`\n",
        "- `padding`: `SAME` or `VALID`.\n",
        "\n",
        "### `pooled = tf.nn.max_pool(value, ksize, strides, padding, data_format='NHWC')`\n",
        "- `pooled` and `value` are both 4-D tensor of `data_format`.\n",
        "- `ksize` and `strides` are both a list of `ints`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "jpShiHtX9E1A"
      },
      "outputs": [],
      "source": [
        "class TextClassificationModel:\n",
        "    \"\"\"\n",
        "    Model class.\n",
        "    reference: http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/\n",
        "    \"\"\"\n",
        "    def __init__(self, args):\n",
        "        self.l2_reg = args.REGULARIZATION\n",
        "        self.embedding_size = args.EMBED_SIZE\n",
        "        self.batch_size = args.BATCH_SIZE\n",
        "        # for each filter size, how many filters we have\n",
        "        self.num_filters = args.NUM_FILTERS\n",
        "        self.filter_sizes = args.FILTER_SIZES\n",
        "        self.num_classes = args.NUM_CLASSES\n",
        "        self.vocab_size = args.VOCAB_SIZE + 2\n",
        "        self.seq_length = args.MAX_SEQ_LENGTH\n",
        "        self.global_step = tf.Variable(initial_value=0, dtype=tf.int32, trainable=False, name='global_step')\n",
        "        self.best_accuracy = tf.Variable(initial_value=0.0, dtype=tf.float32, trainable=False, name='best_accuracy')\n",
        "        \n",
        "        self.input_X = tf.placeholder(dtype=tf.int32, shape=[None, self.seq_length], name='input_X')\n",
        "        self.output_y = tf.placeholder(dtype=tf.int32, shape = [None, self.num_classes], name='output_y')\n",
        "        # when training, feed keep_prob; when testing, feed 1.0\n",
        "        self.keep_prob = tf.placeholder(dtype=tf.float32, name='keep_prob')\n",
        "        \n",
        "        self.embedding_inputs = self.embedding_layer(self.input_X)\n",
        "        self.conv_res = self.conv_max_pool_layer(self.embedding_inputs)\n",
        "        self.scores, self.predictions, self.accuracy = self.score_layer(self.conv_res)\n",
        "        self.cost = self.cost_layer(self.scores)\n",
        "        self.optimize = self.optimizer(self.cost)\n",
        "        \n",
        "    \n",
        "    def embedding_layer(self, word_inputs):\n",
        "        with tf.variable_scope('word_embedding', initializer=tf.contrib.layers.xavier_initializer()), tf.device('/cpu:0'):\n",
        "            embedding_matrix = tf.get_variable(name='embedding_matrix', shape=[self.vocab_size, self.embedding_size])\n",
        "            inputs = tf.nn.embedding_lookup(params=embedding_matrix, ids=word_inputs, name='embed')\n",
        "            print('inputs shape: ', inputs.get_shape()) # shape: (None, seq_length, embedding_size)\n",
        "            # expand one dimension at last dimension: axis=-1\n",
        "            inputs_expanded = tf.expand_dims(input=inputs, axis=-1)  \n",
        "            print('inputs_expanded shape: ', inputs_expanded.get_shape()) # shape: (None, seq_length, embedding_size, 1)\n",
        "        return inputs_expanded\n",
        "    \n",
        "    def conv_max_pool_layer(self, embed_input):\n",
        "        pooled_outputs = []\n",
        "        for idx, filter_size in enumerate(self.filter_sizes):\n",
        "            with tf.variable_scope('conv-maxpool-%s' % filter_size):\n",
        "                # initialization from https://www.tensorflow.org/programmers_guide/variables\n",
        "                # shape: (filter_height, filter_width, in_channels, out_channels)\n",
        "                filter_W = tf.get_variable(name='filter_W', shape=[filter_size, self.embedding_size, 1, self.num_filters], initializer=tf.random_normal_initializer())\n",
        "                filter_b = tf.get_variable(name='filter_b', shape=[self.num_filters], initializer=tf.constant_initializer(0.0))\n",
        "                conv = tf.nn.conv2d(input=embed_input, filter=filter_W, strides=[1, 1, 1, 1], padding='VALID', name='conv')\n",
        "                conv_activation = tf.nn.relu(tf.nn.bias_add(conv, filter_b), name='relu')\n",
        "                print('filter_size:{}, conv_activation shape:{}'.format(filter_size, conv_activation.get_shape()))\n",
        "                # max-pooling\n",
        "                pooled = tf.nn.max_pool(value=conv_activation, ksize=[1, self.seq_length-filter_size+1, 1, 1], strides=[1, 1, 1, 1], padding='VALID', name='max-pool')\n",
        "                print('filter_size:{}, pooled shape:{}'.format(filter_size, pooled.get_shape()))\n",
        "                pooled_outputs.append(pooled)\n",
        "        conv_pooled = tf.concat(values=pooled_outputs, axis=-1)\n",
        "        print('='*100)\n",
        "        print('conv_pooled shape: ', conv_pooled.get_shape()) # shape: (None, 1, 1, self.num_filters*len(self.filter_sizes))\n",
        "        conv_pooled_flat = tf.reshape(conv_pooled, [-1, self.num_filters*len(self.filter_sizes)]) \n",
        "        print('conv_pooled_flat shape: ', conv_pooled_flat.get_shape()) # shape: (None, self.num_filters*len(self.filter_sizes)\n",
        "        return conv_pooled_flat\n",
        "            \n",
        "    def score_layer(self, conv_res):\n",
        "        with tf.variable_scope('score'):\n",
        "            # fc1 with He initialization\n",
        "            he_init = tf.contrib.layers.variance_scaling_initializer()\n",
        "            fc1 = tf.contrib.layers.fully_connected(inputs=conv_res, \n",
        "                                                    num_outputs=self.num_filters*len(self.filter_sizes),\n",
        "                                                    weights_initializer=he_init,\n",
        "                                                    weights_regularizer=tf.contrib.layers.l2_regularizer(scale=self.l2_reg))\n",
        "            print('fc1 shape: ', fc1.get_shape()) # shape: (None, self.num_filters*len(self.filter_sizes))\n",
        "                        \n",
        "            fc1_drop = tf.nn.dropout(fc1, self.keep_prob)\n",
        "            \n",
        "            scores = tf.contrib.layers.fully_connected(inputs=fc1_drop, \n",
        "                                                       num_outputs=self.num_classes, \n",
        "                                                       activation_fn=None,\n",
        "                                                       weights_regularizer=tf.contrib.layers.l2_regularizer(scale=self.l2_reg))\n",
        "            print('scores shape: ', scores.get_shape()) # shape: (None, self.num_classes)\n",
        "            probs = tf.nn.softmax(scores)\n",
        "            predictions = tf.argmax(probs, 1, name='predictions')\n",
        "            print('predictions shape: ', predictions.get_shape()) # shape: (None, )\n",
        "            accuracy = tf.reduce_mean(tf.cast(tf.equal(predictions, tf.argmax(self.output_y, 1)), tf.float32))\n",
        "            tf.summary.scalar(name='accuracy', tensor=accuracy)\n",
        "        return scores, predictions, accuracy\n",
        "        \n",
        "    \n",
        "    def cost_layer(self, scores):\n",
        "        with tf.variable_scope('cost'):\n",
        "            cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=scores, labels=self.output_y))\n",
        "            # https://stackoverflow.com/questions/37107223/how-to-add-regularizations-in-tensorflow\n",
        "            reg_ws = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
        "            for w in reg_ws:\n",
        "                shp = w.get_shape().as_list()\n",
        "                print('{} shape: {} size: {}'.format(w.name, shp, np.prod(shp)))\n",
        "            cost = cost + tf.reduce_sum(reg_ws)\n",
        "            tf.summary.scalar(name='loss', tensor=cost)\n",
        "            self.summary_op = tf.summary.merge_all()\n",
        "        return cost\n",
        "    \n",
        "    def optimizer(self, cost):\n",
        "        with tf.name_scope('optimizer'):\n",
        "            optimizer = tf.train.AdamOptimizer(learning_rate=1e-3)\n",
        "            optimize = optimizer.minimize(loss=cost, global_step=self.global_step)\n",
        "        return optimize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1eYk8Sc9E1D"
      },
      "source": [
        "`train` method to train model.\n",
        "\n",
        "`train_writer` and `test_writer` used as indicator of `accuracy` and `loss` for training and testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "2D3PwlYM9E1E"
      },
      "outputs": [],
      "source": [
        "def train(data, model, args):\n",
        "    saver = tf.train.Saver()\n",
        "    with tf.Session() as sess:\n",
        "        train_writer = tf.summary.FileWriter(logdir=args.LOGDIR + '/train', graph=sess.graph)\n",
        "        test_writer = tf.summary.FileWriter(logdir=args.LOGDIR + '/test')\n",
        "        \n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        ckpt = tf.train.get_checkpoint_state(checkpoint_dir=args.CHECKPOINTS_DIR)\n",
        "        if ckpt and ckpt.model_checkpoint_path:\n",
        "            saver.restore(sess=sess, save_path=ckpt.model_checkpoint_path)\n",
        "            print(ckpt)\n",
        "        \n",
        "        max_iteration_num = args.NUM_EPOCH * data.TRAINING_SIZE // args.BATCH_SIZE        \n",
        "        for idx in range(1, max_iteration_num):\n",
        "            batch_X, batch_y = data.next_batch_training()\n",
        "            \n",
        "            feed_dict = {model.input_X: batch_X, model.output_y: batch_y, model.keep_prob: args.KEEP_PROB}\n",
        "            tmp_accuracy, tmp_cost, _, tmp_summary = sess.run([model.accuracy, model.cost, model.optimize, model.summary_op], feed_dict=feed_dict)\n",
        "            train_writer.add_summary(summary=tmp_summary, global_step=model.global_step.eval())\n",
        "            \n",
        "            if idx % 50 == 0:\n",
        "                print('='*100)\n",
        "                print('Step: {} / {}, training loss:{:4f}, training accuracy:{:4f}'.format(idx, max_iteration_num, tmp_cost, tmp_accuracy))\n",
        "                \n",
        "            if idx % 200 == 0:\n",
        "                test_cost = 0.0\n",
        "                test_accuracy = 0.0\n",
        "                cc = 0\n",
        "                test_batch_X, test_batch_y = data.testing_data()\n",
        "                test_feed_dict = {model.input_X: test_batch_X, model.output_y: test_batch_y, model.keep_prob:1.0}\n",
        "                test_tmp_cost, test_tmp_accuracy, test_tmp_summary = sess.run([model.cost, model.accuracy, model.summary_op], feed_dict=test_feed_dict)\n",
        "                test_writer.add_summary(summary=test_tmp_summary, global_step=model.global_step.eval())\n",
        "                print('-'*100)\n",
        "                #print('Step:{}, testing accuracy:{:4f}'.format(model.global_step.eval(), test_tmp_accuracy))\n",
        "                print('Step: {} / {}, testing loss:{:4f}, testing accuracy:{:4f}'.format(idx, max_iteration_num, test_tmp_cost, test_tmp_accuracy))\n",
        "                if test_tmp_accuracy > model.best_accuracy.eval():\n",
        "                    print('Best model accuracy: {:4f}'.format(test_tmp_accuracy))\n",
        "                    sess.run(model.best_accuracy.assign(test_tmp_accuracy))\n",
        "                    saver.save(sess=sess, save_path=os.path.join(args.CHECKPOINTS_DIR, 'text_classification_cnn.ckpt'), global_step=model.global_step.eval())\n",
        "                else:                    \n",
        "                    print('Best model accuracy: {:4f}, Current model accuracy: {:4f}'.format(model.best_accuracy.eval(), test_tmp_accuracy))\n",
        "                    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poSU3m6g9E1F"
      },
      "source": [
        "`test` method to load the trained model and test model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "eFGuing-9E1F"
      },
      "outputs": [],
      "source": [
        "def test(data, model, args):\n",
        "    categories = ['auto', 'business', 'IT', 'health', 'sports', 'yule']\n",
        "    saver = tf.train.Saver()    \n",
        "    with tf.Session() as sess:\n",
        "        ckpt = tf.train.get_checkpoint_state(checkpoint_dir=args.CHECKPOINTS_DIR)\n",
        "        if ckpt and ckpt.model_checkpoint_path:\n",
        "            saver.restore(sess=sess, save_path=ckpt.model_checkpoint_path)\n",
        "            print(ckpt)\n",
        "        test_X, test_y = data.testing_data()\n",
        "        print('test_X shape: ', test_X.shape)\n",
        "        print('test_y shape: ', test_y.shape)\n",
        "        test_feed_dict = {model.input_X: test_X, model.output_y:test_y, model.keep_prob:1.0}\n",
        "        test_predictions = sess.run(model.predictions, feed_dict=test_feed_dict)\n",
        "        print('Precision, Recall and F1-Score:')\n",
        "        test_y = np.argmax(test_y, 1)\n",
        "        print(metrics.classification_report(test_y, test_predictions, target_names=categories))\n",
        "\n",
        "        print('Confusion matrix:')\n",
        "        cm = metrics.confusion_matrix(test_y, test_predictions)\n",
        "        print(cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "na21PC_79E1G"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    args = Arguments()\n",
        "    data = DataGenerator(args)\n",
        "    model = TextClassificationModel(args)\n",
        "    \n",
        "    # for training\n",
        "    #train(data, model, args)   \n",
        "    \n",
        "    # testing\n",
        "    test(data, model, args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "eTr4iPkc9E1H"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.3"
    },
    "colab": {
      "name": "CNN_text_classification.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}