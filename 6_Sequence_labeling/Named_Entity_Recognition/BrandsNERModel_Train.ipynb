{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/onlyabhilash/NLP-Projects/blob/main/6_Sequence_labeling/Named_Entity_Recognition/BrandsNERModel_Train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "xSTyLhmE9uxw"
      },
      "outputs": [],
      "source": [
        "# %load train.py\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import itertools\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "\n",
        "from model import BrandsNERModel\n",
        "from conlleval import return_report\n",
        "from utils_train import BatchManager, prepare_dataset, update_tag_scheme\n",
        "from utils_io import clean, make_path, load_sentences, augment_with_pretrained, tag_mapping, char_mapping, load_config, save_config, load_word2vec\n",
        "\n",
        "flags = tf.app.flags\n",
        "\n",
        "# input\n",
        "flags.DEFINE_string('data_dir',          'data',   'Path for training, development, testing and embedding data.')\n",
        "\n",
        "# output\n",
        "flags.DEFINE_string('summary_dir',  'summaries',   'Path for training and testing summaries.')\n",
        "flags.DEFINE_string('ckpt_dir',     'checkpoints',  'Path for saving model checkpoints.')\n",
        "\n",
        "# pre-processing\n",
        "flags.DEFINE_boolean('zeros',            True,       'Replace digits with zero.')\n",
        "flags.DEFINE_boolean('lower',            True,       'Convert character to lower case.')\n",
        "flags.DEFINE_string('tag_schema',     'iobes',    'Tagging schema iobes or iob')\n",
        "\n",
        "# bi-directional lstm + crf model\n",
        "flags.DEFINE_integer('word_dim',            20,        'Embedding dimension for word, 0 if not used.')\n",
        "flags.DEFINE_integer('char_dim',           100,        'Embedding dimension for character.')\n",
        "flags.DEFINE_integer('num_units',          100,        'Number of recurrent units in LSTM cell.')\n",
        "\n",
        "# training\n",
        "flags.DEFINE_float('learning_rate',       0.001,      'Initial learning rate.')\n",
        "flags.DEFINE_float('max_gradient_norm',       5,      'Clip gradients to this norm.')\n",
        "flags.DEFINE_float('batch_size',             20,      'Batch size to use during training.')\n",
        "flags.DEFINE_float('keep_prop',             0.5,      'Initial dropout rate.')\n",
        "flags.DEFINE_boolean('use_crf',            False,      'Use crf layer or softmax layer as the top layer.')\n",
        "flags.DEFINE_integer('num_epoch',           100,      'Number of epochs.')\n",
        "\n",
        "# util\n",
        "flags.DEFINE_boolean('clean',              True,      'Clean all the training-related folders and files.')\n",
        "\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "def create_model(session, Model_class, path, load_vec, config, id_to_char):\n",
        "    \"\"\"\n",
        "    Train the new model or re-use trained model.\n",
        "    \"\"\"\n",
        "    model = Model_class(config)\n",
        "    ckpt = tf.train.get_checkpoint_state(checkpoint_dir=path)\n",
        "    if ckpt and ckpt.model_checkpoint_path:\n",
        "        print('Reading model parameters from %s' % ckpt.model_checkpoint_path)\n",
        "        model.saver.restore(session, ckpt.model_checkpoint_path)\n",
        "    else:\n",
        "        print('Created model with fresh parameters.')\n",
        "        session.run(tf.global_variables_initializer())\n",
        "        # assign character embeddings\n",
        "        emb_weights = session.run(model.char_embeddings.read_value())\n",
        "        emb_weights = load_vec(config['character_embedding_file'], id_to_char, config['char_dim'], emb_weights)\n",
        "        session.run(model.char_embeddings.assign(emb_weights))\n",
        "    return model\n",
        "\n",
        "\n",
        "def train(training_file_name, dev_file_name, test_file_name, maps_file_name, character_embedding_file_name, config_file_name):\n",
        "    \"\"\"\n",
        "    Train main entrance.\n",
        "    \"\"\"\n",
        "    training_file = os.path.join(FLAGS.data_dir, training_file_name)\n",
        "    dev_file = os.path.join(FLAGS.data_dir, dev_file_name)\n",
        "    test_file = os.path.join(FLAGS.data_dir, test_file_name)\n",
        "    maps_file = os.path.join(FLAGS.data_dir, maps_file_name)\n",
        "    embedding_file = os.path.join(FLAGS.data_dir, character_embedding_file_name)\n",
        "    config_file = os.path.join(FLAGS.data_dir, config_file_name)\n",
        "\n",
        "    # load data sets\n",
        "    # brands.train, dev, test tagging schema: IOB\n",
        "    # train_sentences format: [[['a', 'B-SHOE'], ['b', 'I-SHOE'], ['c', I-SHOE], ['d', 'O'], ...], [next training example data]]\n",
        "    train_sentences = load_sentences(training_file, FLAGS.zeros)\n",
        "    dev_sentences = load_sentences(dev_file, FLAGS.zeros)\n",
        "    test_sentences = load_sentences(test_file, FLAGS.zeros)\n",
        "\n",
        "    # Use selected tagging scheme (IOB / IOBES)\n",
        "    # train_sentences format: [[['a', 'B-SHOE'], ['b', 'I-SHOE'], ['c', E-SHOE], ['d', 'O'], ...], [next training example data]]\n",
        "    update_tag_scheme(train_sentences, FLAGS.tag_schema)\n",
        "    update_tag_scheme(test_sentences, FLAGS.tag_schema)\n",
        "\n",
        "    # create maps.pkl if not exist\n",
        "    # maps.pkl contains: char_to_id, id_to_char, tag_to_id, id_to_tag\n",
        "    if not os.path.isfile(maps_file):\n",
        "        print('create map file')\n",
        "        # create dictionary for each character\n",
        "        dict_chars_train = char_mapping(train_sentences, FLAGS.lower)[0]\n",
        "        # update dictionary by add the characters in embedding files or test data set\n",
        "        dict_chars, char_to_id, id_to_char = augment_with_pretrained(dict_chars_train.copy(), embedding_file, list(itertools.chain.from_iterable([[w[0] for w in s] for s in dev_sentences])))\n",
        "        # Create a dictionary and a mapping for tags\n",
        "        _t, tag_to_id, id_to_tag = tag_mapping(train_sentences)\n",
        "        # pickle data\n",
        "        with open(maps_file, 'wb') as f:\n",
        "            pickle.dump([char_to_id, id_to_char, tag_to_id, id_to_tag], f)\n",
        "    else:\n",
        "        print('load map file')\n",
        "        with open(maps_file, 'rb') as f:\n",
        "            char_to_id, id_to_char, tag_to_id, id_to_tag = pickle.load(f)\n",
        "\n",
        "    # convert character, tag, word segmentation to id\n",
        "    train_data = prepare_dataset(train_sentences, char_to_id, tag_to_id, FLAGS.lower)\n",
        "    dev_data = prepare_dataset(dev_sentences, char_to_id, tag_to_id, FLAGS.lower)\n",
        "    test_data = prepare_dataset(test_sentences, char_to_id, tag_to_id, FLAGS.lower)\n",
        "    print('%i / %i / %i sentences in train / dev / test.' % (len(train_data), len(dev_data), len(test_data)))\n",
        "\n",
        "    # prepare mini-batch data\n",
        "    train_manager = BatchManager(train_data, FLAGS.batch_size)\n",
        "    dev_manager = BatchManager(dev_data, 100)\n",
        "    test_manager = BatchManager(test_data, 100)\n",
        "\n",
        "    # make path for store summary and model if not exist\n",
        "    make_path(FLAGS)\n",
        "\n",
        "    if os.path.isfile(config_file):\n",
        "        config = load_config(config_file)\n",
        "    else:\n",
        "        config = OrderedDict()\n",
        "        config['num_chars'] = len(char_to_id)\n",
        "        config['char_dim'] = FLAGS.char_dim\n",
        "        config['num_tags'] = len(tag_to_id)\n",
        "        config['word_dim'] = FLAGS.word_dim\n",
        "        config['num_units'] = FLAGS.num_units\n",
        "        config['batch_size'] = FLAGS.batch_size\n",
        "        config['character_embedding_file'] = os.path.join(FLAGS.data_dir, character_embedding_file_name)\n",
        "        config['max_gradient_norm'] = FLAGS.max_gradient_norm\n",
        "        config['keep_prop'] = FLAGS.keep_prop\n",
        "        config['learning_rate'] = FLAGS.learning_rate\n",
        "        config['zeros'] = FLAGS.zeros\n",
        "        config['lower'] = FLAGS.lower\n",
        "        config['use_crf'] = FLAGS.use_crf\n",
        "        save_config(config, config_file)\n",
        "\n",
        "    # config parameters for the tf.Session\n",
        "    tf_config = tf.ConfigProto()\n",
        "    tf_config.gpu_options.allow_growth = True\n",
        "    # number of mini-batches per epoch\n",
        "    steps_per_epoch = train_manager.len_data\n",
        "\n",
        "    with tf.Session(config=tf_config) as sess:\n",
        "        # train_writer = tf.summary.FileWriter(logdir=os.path.join(FLAGS.summary_dir, 'train'), graph=sess.graph)\n",
        "        # test_writer = tf.summary.FileWriter(logdir=os.path.join(FLAGS.summary_dir, 'test'), graph=sess.graph)\n",
        "        train_writer = tf.summary.FileWriter(logdir=FLAGS.summary_dir, graph=sess.graph)\n",
        "        model = create_model(sess, BrandsNERModel, FLAGS.ckpt_dir, load_word2vec, config, id_to_char)\n",
        "        loss = []\n",
        "        for i in range(FLAGS.num_epoch):\n",
        "            for mini_batch in train_manager.iter_batch(shuffle=True):\n",
        "                global_step, mini_batch_cost, mini_batch_summary = model.step(sess, mini_batch, is_training=True, keep_prop=FLAGS.keep_prop)\n",
        "                train_writer.add_summary(summary=mini_batch_summary, global_step=global_step)\n",
        "                loss.append(mini_batch_cost)\n",
        "                if global_step % 100 == 0:\n",
        "                    print('iteration:{} step:{}/{}, NER loss:{:>9.6f}'.format(i+1, global_step%steps_per_epoch, steps_per_epoch, np.mean(loss)))\n",
        "                    loss = []\n",
        "            # evaluate the model on development data\n",
        "            best = evaluate(sess, model, 'dev', dev_manager, id_to_tag)\n",
        "            # if have better dev F1 score until now, then save the model\n",
        "            if best:\n",
        "                model.saver.save(sess=sess, save_path=os.path.join(FLAGS.ckpt_dir, 'Brands_ner.ckpt'), global_step=model.global_step.eval())\n",
        "            # report the test F1 score\n",
        "            evaluate(sess, model, 'test', test_manager, id_to_tag)\n",
        "\n",
        "\n",
        "def evaluate(sess, model, name, data, id_to_tag):\n",
        "    print('====================== evaluate:{}'.format(name))\n",
        "\n",
        "    # ner_results contains 'character - real tag - predicted tag' for all samples in 'data'\n",
        "    ner_results = model.evaluate(sess, data, id_to_tag)\n",
        "    eval_lines = test_ner(ner_results, FLAGS.data_dir)\n",
        "\n",
        "    for line in eval_lines:\n",
        "        print(line)\n",
        "    f1 = float(eval_lines[1].strip().split()[-1])\n",
        "\n",
        "    if name == 'dev':\n",
        "        best_test_f1 = model.best_dev_f1.eval()\n",
        "        if f1 > best_test_f1:\n",
        "            sess.run(model.best_dev_f1.assign(f1))\n",
        "            print('new best dev f1 score:{:>.3f}'.format(f1))\n",
        "        return f1 > best_test_f1\n",
        "    elif name == 'test':\n",
        "        best_test_f1 = model.best_test_f1.eval()\n",
        "        if f1 > best_test_f1:\n",
        "            sess.run(model.best_test_f1.assign(f1))\n",
        "            print('new best test f1 score:{:>.3f}'.format(f1))\n",
        "        return f1 > best_test_f1\n",
        "\n",
        "def test_ner(results, path):\n",
        "    \"\"\"\n",
        "    Report the performance.\n",
        "    \"\"\"\n",
        "    output_file = os.path.join(path, 'Brands_ner_predict.utf8')\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        to_write = []\n",
        "        for block in results:\n",
        "            for line in block:\n",
        "                to_write.append(line + '\\n')\n",
        "            to_write.append('\\n')\n",
        "\n",
        "        f.writelines(to_write)\n",
        "    eval_lines = return_report(output_file)\n",
        "    return eval_lines\n",
        "\n",
        "def main(_):\n",
        "    if FLAGS.clean:\n",
        "        clean(FLAGS, 'maps.pkl', 'BrandsNERModel.config', 'Brands_ner_predict.utf8')\n",
        "    train('brands.train', 'brands.dev', 'brands.test', 'maps.pkl', 'wiki_100.utf8', 'BrandsNERModel.config')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    tf.app.run(main)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.3"
    },
    "colab": {
      "name": "BrandsNERModel_Train.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}